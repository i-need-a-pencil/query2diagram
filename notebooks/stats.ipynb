{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37c338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from q2d.checker import analyze_errors_by_severity\n",
    "from q2d.common.utils import get_data_id\n",
    "from scipy import stats\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79da231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./datasets/annotations.csv\")\n",
    "dataset[\"sample_id_fixed\"] = dataset.apply(\n",
    "    lambda x: get_data_id(x[\"code\"]) + \"[SEP]\" + get_data_id(x[\"query\"]) + \"[SEP]\" + x[\"version\"], axis=1\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6658b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_to_ids = {\n",
    "    \"GPT-4o\": [\"6851ee157b9b577dc44f1d6d\", \"6851ee257b9b577dc44f1d9e\"],\n",
    "    \"Claude 0-shot\": [\"680ca2aace63f152b3965528\", \"680ca293ce63f152b39654f7\"],\n",
    "    \"Qwen2.5-Coder-14B 0-shot\": [\"680ca307ce63f152b396558a\", \"680ca321ce63f152b39655bb\"],\n",
    "    \"Qwen2.5-Coder-14B SFT Claude Synth\": [\"680ca197ce63f152b3965402\", \"680ca17dce63f152b39653d1\"],\n",
    "    \"Qwen2.5-Coder-14B SFT Fixed Claude Synth\": [\"680ca278ce63f152b39654c6\", \"680ca262ce63f152b3965495\"],\n",
    "}\n",
    "annotators = {\"first\", \"second\", \"agreed\"}\n",
    "current_annotator = \"agreed\"  # first to get kappa_ci_asymptotic value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1330962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kappa_ci_asymptotic(rater1, rater2, alpha=0.05):\n",
    "    rater1, rater2 = np.asarray(rater1), np.asarray(rater2)\n",
    "    n = len(rater1)\n",
    "    kappa = cohen_kappa_score(rater1, rater2)\n",
    "\n",
    "    cats = np.unique(np.concatenate([rater1, rater2]))\n",
    "    cat_to_idx = {cat: i for i, cat in enumerate(cats)}\n",
    "\n",
    "    k = len(cats)\n",
    "    contingency = np.zeros((k, k))\n",
    "    for i in range(n):\n",
    "        contingency[cat_to_idx[rater1[i]], cat_to_idx[rater2[i]]] += 1\n",
    "\n",
    "    p = contingency / n\n",
    "    p_i = p.sum(axis=1)\n",
    "    p_j = p.sum(axis=0)\n",
    "    p_o = np.trace(p)\n",
    "    p_e = np.sum(p_i * p_j)\n",
    "\n",
    "    if np.isclose(p_e, 1):\n",
    "        return kappa, kappa, kappa\n",
    "\n",
    "    var_kappa = (\n",
    "        p_o * (1 - p_o) / ((1 - p_e) ** 2)\n",
    "        + 2 * (1 - p_o) * (2 * p_o * p_e - np.sum(np.diag(p) * (p_i + p_j))) / ((1 - p_e) ** 3)\n",
    "        + (1 - p_o) ** 2 * (np.sum(p * np.power(p_i[:, None] + p_j, 2)) - 4 * p_e**2) / ((1 - p_e) ** 4)\n",
    "    ) / n\n",
    "\n",
    "    z = stats.norm.ppf(1 - alpha / 2)\n",
    "    se = np.sqrt(max(0, var_kappa))\n",
    "\n",
    "    return kappa, max(-1, kappa - z * se), min(1, kappa + z * se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc2e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_nodes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"temp\"] = df[\"nodes\"].apply(lambda x: [(key, val) for key, val in literal_eval(x).items()])\n",
    "    df_exploded = df.explode(\"temp\")\n",
    "    df_exploded[\"label\"] = df_exploded[\"temp\"].apply(lambda x: x[0])\n",
    "    df_exploded[\"node_id\"] = df_exploded[\"temp\"].apply(lambda x: x[1])\n",
    "\n",
    "    df_exploded = df_exploded[df_exploded[\"node_id\"].apply(len) > 0]\n",
    "    df_exploded = df_exploded.explode(\"node_id\")\n",
    "\n",
    "    return df_exploded.drop(columns=[\"temp\"])[[\"sample_id_fixed\", \"node_id\", \"label\", \"repo\", \"version\", \"task_id\"]]\n",
    "\n",
    "\n",
    "all_tasks = set(task for name, tasks in experiment_to_ids.items() for task in tasks)\n",
    "current_annotator_labels = explode_nodes(\n",
    "    dataset[\n",
    "        (dataset[\"annotator\"] == current_annotator)\n",
    "        & (dataset[\"status\"] == \"Finalized\")\n",
    "        & dataset[\"task_id\"].isin(all_tasks)\n",
    "    ][[\"sample_id_fixed\", \"nodes\", \"repo\", \"version\", \"task_id\"]]\n",
    ")\n",
    "\n",
    "for another_annotator in annotators:\n",
    "    if another_annotator != current_annotator:\n",
    "        another_annotator_labels = explode_nodes(\n",
    "            dataset[\n",
    "                (dataset[\"annotator\"] == another_annotator)\n",
    "                & (dataset[\"status\"] == \"Finalized\")\n",
    "                & dataset[\"task_id\"].isin(all_tasks)\n",
    "            ][[\"sample_id_fixed\", \"nodes\", \"repo\", \"version\", \"task_id\"]]\n",
    "        )\n",
    "        merged_df = current_annotator_labels.merge(\n",
    "            another_annotator_labels,\n",
    "            \"inner\",\n",
    "            on=[\"sample_id_fixed\", \"task_id\", \"version\", \"repo\", \"node_id\"],\n",
    "            suffixes=(\"_cur\", \"_another\"),\n",
    "        )\n",
    "        kappa, kappa_min, kappa_max = kappa_ci_asymptotic(merged_df[\"label_cur\"], merged_df[\"label_another\"])\n",
    "        print(f\"Cohen kappa {current_annotator} to {another_annotator}: {kappa} ci: [{kappa_min}, {kappa_max}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8ec1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tasks = set(task for name, tasks in experiment_to_ids.items() for task in tasks)\n",
    "subset = dataset[\n",
    "    (dataset[\"annotator\"] == current_annotator)\n",
    "    & (dataset[\"status\"] == \"Finalized\")\n",
    "    & dataset[\"task_id\"].isin(all_tasks)\n",
    "]\n",
    "stats = subset.apply(lambda x: Counter({key: len(val) for key, val in literal_eval(x[\"nodes\"]).items()}), axis=1)\n",
    "max_stats = pd.DataFrame.from_dict([dict(stat) for stat in stats])\n",
    "max_stats[\"sample_id_fixed\"] = subset[\"sample_id_fixed\"].reset_index(drop=True)\n",
    "max_stats = max_stats.groupby(\"sample_id_fixed\").max().reset_index()\n",
    "\n",
    "assert len(max_stats) == 24 * 2\n",
    "\n",
    "total_max_stats = max_stats[[\"Sufficiency\", \"Completeness\", \"Hallucinations\", \"Verbosity\"]].sum()\n",
    "total_max_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf8058",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_macro_stats = {}\n",
    "all_micro_stats = {}\n",
    "for name, tasks in experiment_to_ids.items():\n",
    "    subset = dataset[\n",
    "        (dataset[\"annotator\"] == current_annotator)\n",
    "        & (dataset[\"status\"] == \"Finalized\")\n",
    "        & dataset[\"task_id\"].isin(tasks)\n",
    "    ]\n",
    "    stats = subset.apply(lambda x: Counter({key: len(val) for key, val in literal_eval(x[\"nodes\"]).items()}), axis=1)\n",
    "\n",
    "    micro_stats = dict(\n",
    "        {\"Sufficiency\": 0, \"Completeness\": 0, \"Verbosity\": 0, \"Hallucinations\": 0} | sum(stats, start=Counter())\n",
    "    )\n",
    "\n",
    "    macro_stats = pd.DataFrame.from_dict([dict(stat) for stat in stats])\n",
    "    macro_stats[\"sample_id_fixed\"] = subset[\"sample_id_fixed\"].reset_index(drop=True)\n",
    "    macro_stats = macro_stats.merge(max_stats, on=\"sample_id_fixed\", suffixes=(\"\", \"_max\"))\n",
    "\n",
    "    all_macro_stats[name] = macro_stats\n",
    "    all_micro_stats[name] = micro_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7065bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for name, _ in experiment_to_ids.items():\n",
    "    micro_stats = all_micro_stats[name]\n",
    "    macro_stats = all_macro_stats[name]\n",
    "\n",
    "    # MICRO\n",
    "    micro_TP = micro_stats[\"Sufficiency\"] + micro_stats[\"Completeness\"]\n",
    "    micro_TP_hard = micro_stats[\"Sufficiency\"]\n",
    "    micro_FP = micro_stats[\"Verbosity\"] + micro_stats[\"Hallucinations\"]\n",
    "    micro_FN = (total_max_stats[\"Sufficiency\"] + total_max_stats[\"Completeness\"]) - (\n",
    "        micro_stats[\"Sufficiency\"] + micro_stats[\"Completeness\"]\n",
    "    )\n",
    "    micro_FN_hard = total_max_stats[\"Sufficiency\"] - micro_stats[\"Sufficiency\"]\n",
    "\n",
    "    micro_precision = round(\n",
    "        micro_TP / (micro_TP + micro_FP),\n",
    "        3,\n",
    "    )\n",
    "    micro_hard_precision = round(\n",
    "        micro_TP_hard / (micro_TP_hard + micro_FP),\n",
    "        3,\n",
    "    )\n",
    "    micro_recall = round(\n",
    "        micro_TP / (micro_TP + micro_FN),\n",
    "        3,\n",
    "    )\n",
    "    micro_hard_recall = round(micro_TP_hard / (micro_TP_hard + micro_FN_hard), 3)\n",
    "\n",
    "    micro_F1 = round(2 * micro_TP / (2 * micro_TP + micro_FP + micro_FN), 3)\n",
    "    micro_hard_F1 = round(2 * micro_TP_hard / (2 * micro_TP_hard + micro_FP + micro_FN), 3)\n",
    "\n",
    "    # MACRO\n",
    "    macro_TP = macro_stats[\"Sufficiency\"] + macro_stats[\"Completeness\"]\n",
    "    macro_TP_hard = macro_stats[\"Sufficiency\"]\n",
    "    macro_FP = macro_stats[\"Verbosity\"] + macro_stats[\"Hallucinations\"]\n",
    "    macro_FN = (macro_stats[\"Sufficiency_max\"] + macro_stats[\"Completeness_max\"]) - (\n",
    "        macro_stats[\"Sufficiency\"] + macro_stats[\"Completeness\"]\n",
    "    )\n",
    "    macro_FN_hard = macro_stats[\"Sufficiency_max\"] - macro_stats[\"Sufficiency\"]\n",
    "\n",
    "    macro_precision = round(\n",
    "        np.mean(\n",
    "            np.nan_to_num(\n",
    "                macro_TP / (macro_TP + macro_FP),\n",
    "                nan=1.0,\n",
    "            )\n",
    "        ),\n",
    "        3,\n",
    "    )\n",
    "    macro_hard_precision = round(\n",
    "        np.mean(\n",
    "            np.nan_to_num(\n",
    "                macro_TP_hard / (macro_TP_hard + macro_FP),\n",
    "                nan=1.0,\n",
    "            )\n",
    "        ),\n",
    "        3,\n",
    "    )\n",
    "\n",
    "    macro_recall = round(\n",
    "        np.mean(\n",
    "            np.nan_to_num(\n",
    "                macro_TP / (macro_TP + macro_FN),\n",
    "                nan=1.0,\n",
    "            )\n",
    "        ),\n",
    "        3,\n",
    "    )\n",
    "    macro_hard_recall = round(np.mean(np.nan_to_num(macro_TP_hard / (macro_TP_hard + macro_FN_hard), nan=1.0)), 3)\n",
    "\n",
    "    macro_F1 = round(np.mean(np.nan_to_num(2 * macro_TP / (2 * macro_TP + macro_FP + macro_FN), nan=1.0)), 3)\n",
    "    macro_hard_F1 = round(\n",
    "        np.mean(np.nan_to_num(2 * macro_TP_hard / (2 * macro_TP_hard + macro_FP + macro_FN), nan=1.0)), 3\n",
    "    )\n",
    "\n",
    "    results.append(\n",
    "        {\"Name\": name}\n",
    "        | dict(micro_stats)\n",
    "        | {\n",
    "            \"Micro_Precision\": micro_precision,\n",
    "            \"Micro_Hard_Precision\": micro_hard_precision,\n",
    "            \"Micro_Recall\": micro_recall,\n",
    "            \"Micro_Hard_Recall\": micro_hard_recall,\n",
    "            \"Micro_F1\": micro_F1,\n",
    "            \"Micro_Hard_F1\": micro_hard_F1,\n",
    "        }\n",
    "        | {\n",
    "            \"Macro_Precision\": macro_precision,\n",
    "            \"Macro_Hard_Precision\": macro_hard_precision,\n",
    "            \"Macro_Recall\": macro_recall,\n",
    "            \"Macro_Hard_Recall\": macro_hard_recall,\n",
    "            \"Macro_F1\": macro_F1,\n",
    "            \"Macro_Hard_F1\": macro_hard_F1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "results_df = pd.DataFrame.from_records(results)\n",
    "print(\n",
    "    results_df[\n",
    "        [\n",
    "            \"Name\",\n",
    "            \"Sufficiency\",\n",
    "            \"Completeness\",\n",
    "            \"Hallucinations\",\n",
    "            \"Verbosity\",\n",
    "        ]\n",
    "    ]\n",
    "    .round(3)\n",
    "    .to_markdown(tablefmt=\"github\", index=False)\n",
    ")\n",
    "print()\n",
    "print(\n",
    "    results_df[\n",
    "        [\n",
    "            \"Name\",\n",
    "            \"Micro_Precision\",\n",
    "            \"Micro_Hard_Precision\",\n",
    "            \"Micro_Recall\",\n",
    "            \"Micro_Hard_Recall\",\n",
    "            \"Micro_F1\",\n",
    "            \"Micro_Hard_F1\",\n",
    "        ]\n",
    "    ]\n",
    "    .round(3)\n",
    "    .to_markdown(tablefmt=\"github\", index=False)\n",
    ")\n",
    "print()\n",
    "print(\n",
    "    results_df[\n",
    "        [\n",
    "            \"Name\",\n",
    "            \"Macro_Precision\",\n",
    "            \"Macro_Hard_Precision\",\n",
    "            \"Macro_Recall\",\n",
    "            \"Macro_Hard_Recall\",\n",
    "            \"Macro_F1\",\n",
    "            \"Macro_Hard_F1\",\n",
    "        ]\n",
    "    ]\n",
    "    .round(3)\n",
    "    .to_markdown(tablefmt=\"github\", index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0889ff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_errors = []\n",
    "unique_errors = []\n",
    "for name, tasks in experiment_to_ids.items():\n",
    "    subset = dataset[\n",
    "        (dataset[\"annotator\"] == current_annotator)\n",
    "        & (dataset[\"status\"] == \"Finalized\")\n",
    "        & dataset[\"task_id\"].isin(tasks)\n",
    "    ]\n",
    "    # total_nodes = sum(subset.apply(lambda x: sum(len(val) for val in literal_eval(x[\"nodes\"]).items()), axis=1))\n",
    "\n",
    "    cur_total = analyze_errors_by_severity(\n",
    "        subset[[\"diagram\", \"code\"]].to_dict(orient=\"records\"), mode=\"total\", average=None\n",
    "    )\n",
    "    # cur_total_mean = {f\"{key}_Mean\":val/total_nodes for key, val in cur_total.items()}\n",
    "    cur_unique = analyze_errors_by_severity(\n",
    "        subset[[\"diagram\", \"code\"]].to_dict(orient=\"records\"), mode=\"unique\", average=None\n",
    "    )\n",
    "    # cur_unique_mean = {f\"{key}_Mean\":val/total_nodes for key, val in cur_unique.items()}\n",
    "\n",
    "    total_errors.append({\"Name\": name} | cur_total)\n",
    "    unique_errors.append({\"Name\": name} | cur_unique)\n",
    "print(\"TOTAL:\")\n",
    "print(\n",
    "    pd.DataFrame.from_records(total_errors)[[\"Name\", \"Low\", \"Medium\", \"High\"]]\n",
    "    .round(3)\n",
    "    .to_markdown(tablefmt=\"github\", index=False)\n",
    ")\n",
    "print()\n",
    "print(\"UNIQUE:\")\n",
    "print(\n",
    "    pd.DataFrame.from_records(unique_errors)[[\"Name\", \"Low\", \"Medium\", \"High\"]]\n",
    "    .round(3)\n",
    "    .to_markdown(tablefmt=\"github\", index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf26098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_errors = []\n",
    "unique_errors = []\n",
    "for name, tasks in experiment_to_ids.items():\n",
    "    subset = dataset[\n",
    "        (dataset[\"annotator\"] == current_annotator)\n",
    "        & (dataset[\"status\"] == \"Finalized\")\n",
    "        & dataset[\"task_id\"].isin(tasks)\n",
    "    ]\n",
    "    # total_nodes = sum(subset.apply(lambda x: sum(len(val) for val in literal_eval(x[\"nodes\"]).items()), axis=1))\n",
    "\n",
    "    cur_total = analyze_errors_by_severity(\n",
    "        subset[[\"diagram\", \"code\"]].to_dict(orient=\"records\"), mode=\"total\", average=\"macro\"\n",
    "    )\n",
    "    # cur_total_mean = {f\"{key}_Mean\":val/total_nodes for key, val in cur_total.items()}\n",
    "    cur_unique = analyze_errors_by_severity(\n",
    "        subset[[\"diagram\", \"code\"]].to_dict(orient=\"records\"), mode=\"unique\", average=\"macro\"\n",
    "    )\n",
    "    # cur_unique_mean = {f\"{key}_Mean\":val/total_nodes for key, val in cur_unique.items()}\n",
    "\n",
    "    total_errors.append({\"Name\": name} | cur_total)\n",
    "    unique_errors.append({\"Name\": name} | cur_unique)\n",
    "print(\"TOTAL:\")\n",
    "print(\n",
    "    pd.DataFrame.from_dict(total_errors)[[\"Name\", \"Low\", \"Medium\", \"High\"]]\n",
    "    .round(3)\n",
    "    .to_markdown(tablefmt=\"github\", index=False)\n",
    ")\n",
    "print()\n",
    "print(\"UNIQUE:\")\n",
    "print(\n",
    "    pd.DataFrame.from_dict(unique_errors)[[\"Name\", \"Low\", \"Medium\", \"High\"]]\n",
    "    .round(3)\n",
    "    .to_markdown(tablefmt=\"github\", index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41226a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_errors = []\n",
    "unique_errors = []\n",
    "for name, tasks in experiment_to_ids.items():\n",
    "    subset = dataset[\n",
    "        (dataset[\"annotator\"] == current_annotator)\n",
    "        & (dataset[\"status\"] == \"Finalized\")\n",
    "        & dataset[\"task_id\"].isin(tasks)\n",
    "    ]\n",
    "    # total_nodes = sum(subset.apply(lambda x: sum(len(val) for val in literal_eval(x[\"nodes\"]).items()), axis=1))\n",
    "\n",
    "    cur_total = analyze_errors_by_severity(\n",
    "        subset[[\"diagram\", \"code\"]].to_dict(orient=\"records\"), mode=\"total\", average=\"micro\"\n",
    "    )\n",
    "    # cur_total_mean = {f\"{key}_Mean\":val/total_nodes for key, val in cur_total.items()}\n",
    "    cur_unique = analyze_errors_by_severity(\n",
    "        subset[[\"diagram\", \"code\"]].to_dict(orient=\"records\"), mode=\"unique\", average=\"micro\"\n",
    "    )\n",
    "    # cur_unique_mean = {f\"{key}_Mean\":val/total_nodes for key, val in cur_unique.items()}\n",
    "\n",
    "    total_errors.append({\"Name\": name} | cur_total)\n",
    "    unique_errors.append({\"Name\": name} | cur_unique)\n",
    "print(\"TOTAL:\")\n",
    "print(\n",
    "    pd.DataFrame.from_records(total_errors)[[\"Name\", \"Low\", \"Medium\", \"High\"]]\n",
    "    .round(3)\n",
    "    .to_markdown(tablefmt=\"github\", index=False)\n",
    ")\n",
    "print()\n",
    "print(\"UNIQUE:\")\n",
    "print(\n",
    "    pd.DataFrame.from_records(unique_errors)[[\"Name\", \"Low\", \"Medium\", \"High\"]]\n",
    "    .round(3)\n",
    "    .to_markdown(tablefmt=\"github\", index=False)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
